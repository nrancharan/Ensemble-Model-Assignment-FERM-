{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Assignment: Norberto Rancharan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For this assignment use data at: “https://www.kaggle.com/wendykan/lending-club-loan-data/download”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn import ensemble\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "#Plotly visualizations\n",
    "from plotly import tools\n",
    "import plotly.plotly as py\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 600)\n",
    "pd.set_option('display.max_rows', 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle for Faster Reading In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = pickle.load(open(\"model_Data.pickle\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data['Delinquent'] = model_data['loan_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = model_data.drop(columns=['loan_status'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set ratio \n",
      " 0    0.694183\n",
      "1    0.105817\n",
      "Name: Delinquent, dtype: float64\n",
      "Test set ratio \n",
      " 0    0.173546\n",
      "1    0.026454\n",
      "Name: Delinquent, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "stratified = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_set, test_set in stratified.split(model_data, model_data[\"Delinquent\"]):\n",
    "    stratified_train = model_data.loc[train_set]\n",
    "    stratified_test = model_data.loc[test_set]\n",
    "    \n",
    "print('Train set ratio \\n', stratified_train[\"Delinquent\"].value_counts()/len(model_data))\n",
    "print('Test set ratio \\n', stratified_test[\"Delinquent\"].value_counts()/len(model_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = stratified_train\n",
    "test_df = stratified_test\n",
    "\n",
    "\n",
    "# Let's Shuffle the data\n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "test_df = test_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Train set (Normal training dataset)\n",
    "X_train = train_df.drop(\"Delinquent\", axis=1)\n",
    "y_train = train_df[\"Delinquent\"]\n",
    "\n",
    "\n",
    "# Test Dataset\n",
    "X_test = test_df.drop(\"Delinquent\", axis=1)\n",
    "y_test = test_df[\"Delinquent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features = ['grade', \n",
    "                      'hardship_last_payment_amount',\n",
    "                      'inq_last_12m',\n",
    "                      'acc_open_past_24mths',\n",
    "                      'open_il_24m',\n",
    "                      'inq_last_6mths',\n",
    "                      'hardship_payoff_balance_amount',\n",
    "                      'loan_amount',\n",
    "                      'all_util',\n",
    "                      'avg_cur_bal',\n",
    "                      'annual_income',\n",
    "                      'dti']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Copies of Test and Train of X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_copy = X_test.copy()\n",
    "X_train_copy = X_train.copy()\n",
    "y_test_copy = y_test.copy()\n",
    "y_train_copy = y_train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Important Features Identified from Grid Search in the Ensemble project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainNN = X_train_copy[important_features]\n",
    "X_testNN = X_test_copy[important_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the Train and Test to Array to use in NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_testNN = X_testNN.values\n",
    "X_trainNN = X_trainNN.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trainNN = y_train_copy.values\n",
    "y_testNN = y_test_copy.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_trainNN = sc.fit_transform(X_trainNN)\n",
    "X_testNN = sc.transform(X_testNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.52726338, -0.06854699,  1.26920797, ...,  0.1812274 ,\n",
       "        -0.07300007, -0.02748638],\n",
       "       [ 1.32236469, -0.06854699, -0.78789298, ...,  0.17945548,\n",
       "         0.04019562, -0.02748582],\n",
       "       [-1.06293924, -0.06854699, -0.78789294, ...,  0.17862231,\n",
       "        -0.2160115 , -0.02748171],\n",
       "       ...,\n",
       "       [-0.26783793, -0.06854699, -0.78789294, ...,  0.17938528,\n",
       "        -0.15789683, -0.0274842 ],\n",
       "       [ 1.32236469, -0.06854699, -0.78789296, ...,  0.17876065,\n",
       "        -0.00508266, -0.02748569],\n",
       "       [-0.26783793, -0.06854699, -0.78789298, ...,  0.17976103,\n",
       "        -0.01640223, -0.02748193]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_testNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.python.keras import backend as k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first step: create a Sequential object, as a sequence of layers. B/C NN is a sequence of layers.\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the first hidden layer\n",
    "classifier.add(Dense(units=5,kernel_initializer='glorot_uniform',\n",
    "                    activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the second hidden layer\n",
    "classifier.add(Dense(units=5,kernel_initializer='glorot_uniform',\n",
    "                    activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the output layer\n",
    "classifier.add(Dense(units=1,kernel_initializer='glorot_uniform',\n",
    "                    activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the NN\n",
    "classifier.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1808534 samples\n",
      "Epoch 1/20\n",
      "1808534/1808534 [==============================] - 200s 111us/sample - loss: 0.3588 - accuracy: 0.8682\n",
      "Epoch 2/20\n",
      "1808534/1808534 [==============================] - 204s 113us/sample - loss: 0.3580 - accuracy: 0.8684\n",
      "Epoch 3/20\n",
      "1808534/1808534 [==============================] - 212s 117us/sample - loss: 0.3579 - accuracy: 0.8684\n",
      "Epoch 4/20\n",
      "1808534/1808534 [==============================] - 189s 104us/sample - loss: 0.3579 - accuracy: 0.8684\n",
      "Epoch 5/20\n",
      "1808534/1808534 [==============================] - 191s 106us/sample - loss: 0.3578 - accuracy: 0.8684\n",
      "Epoch 6/20\n",
      "1808534/1808534 [==============================] - 192s 106us/sample - loss: 0.3578 - accuracy: 0.8684\n",
      "Epoch 7/20\n",
      "1808534/1808534 [==============================] - 191s 106us/sample - loss: 0.3578 - accuracy: 0.8684\n",
      "Epoch 8/20\n",
      "1808534/1808534 [==============================] - 194s 107us/sample - loss: 0.3578 - accuracy: 0.8684\n",
      "Epoch 9/20\n",
      "1808534/1808534 [==============================] - 188s 104us/sample - loss: 0.3577 - accuracy: 0.8684\n",
      "Epoch 10/20\n",
      "1808534/1808534 [==============================] - 190s 105us/sample - loss: 0.3578 - accuracy: 0.8684\n",
      "Epoch 11/20\n",
      "1808534/1808534 [==============================] - 187s 104us/sample - loss: 0.3578 - accuracy: 0.8684\n",
      "Epoch 12/20\n",
      "1808534/1808534 [==============================] - 192s 106us/sample - loss: 0.3578 - accuracy: 0.8684\n",
      "Epoch 13/20\n",
      "1808534/1808534 [==============================] - 196s 108us/sample - loss: 0.3579 - accuracy: 0.8684\n",
      "Epoch 14/20\n",
      "1808534/1808534 [==============================] - 189s 104us/sample - loss: 0.3579 - accuracy: 0.8684\n",
      "Epoch 15/20\n",
      "1808534/1808534 [==============================] - 203s 112us/sample - loss: 0.3579 - accuracy: 0.8684\n",
      "Epoch 16/20\n",
      "1808534/1808534 [==============================] - 206s 114us/sample - loss: 0.3579 - accuracy: 0.8684\n",
      "Epoch 17/20\n",
      "1808534/1808534 [==============================] - 190s 105us/sample - loss: 0.3580 - accuracy: 0.8684\n",
      "Epoch 18/20\n",
      "1808534/1808534 [==============================] - 187s 103us/sample - loss: 0.3579 - accuracy: 0.8684\n",
      "Epoch 19/20\n",
      "1808534/1808534 [==============================] - 189s 104us/sample - loss: 0.3579 - accuracy: 0.8684\n",
      "Epoch 20/20\n",
      "1808534/1808534 [==============================] - 188s 104us/sample - loss: 0.3579 - accuracy: 0.8684\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2198b1bc748>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "classifier.fit(X_trainNN,y_trainNN,batch_size=10,epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_testNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.11002514],\n",
       "       [0.0269669 ],\n",
       "       [0.15469787],\n",
       "       ...,\n",
       "       [0.12921911],\n",
       "       [0.02737189],\n",
       "       [0.12034363]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (y_pred>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_testNN, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[391420,    910],\n",
       "       [ 58552,   1252]], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# k-fold clustering\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier():\n",
    "    # first step: create a Sequential object, as a sequence of layers. B/C NN is a sequence of layers.\n",
    "    classifier = Sequential()\n",
    "    # add the first hidden layer\n",
    "    classifier.add(Dense(units=5,kernel_initializer='glorot_uniform',\n",
    "                    activation = 'relu'))\n",
    "    # add the second hidden layer\n",
    "    classifier.add(Dense(units=5,kernel_initializer='glorot_uniform',\n",
    "                    activation = 'relu'))\n",
    "    # add the output layer\n",
    "    classifier.add(Dense(units=1,kernel_initializer='glorot_uniform',\n",
    "                    activation = 'sigmoid'))\n",
    "    # compiling the NN\n",
    "    classifier.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1627680 samples\n",
      "1627680/1627680 [==============================] - 272s 167us/sample - loss: 0.3587 - acc: 0.8683: 1s - loss: - ETA: 0s - loss: 0.358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1627680 samples\n",
      "1627680/1627680 [==============================] - 270s 166us/sample - loss: 0.3595 - acc: 0.8680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1627680 samples\n",
      "1627680/1627680 [==============================] - 264s 162us/sample - loss: 0.3588 - acc: 0.8681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1627680 samples\n",
      "1627680/1627680 [==============================] - 258s 159us/sample - loss: 0.3586 - acc: 0.8683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 289s 178us/sample - loss: 0.3589 - acc: 0.8678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 292s 179us/sample - loss: 0.3591 - acc: 0.8682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 309s 190us/sample - loss: 0.3588 - acc: 0.8684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 279s 172us/sample - loss: 0.3589 - acc: 0.8681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 279s 172us/sample - loss: 0.3588 - acc: 0.8680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 279s 171us/sample - loss: 0.3592 - acc: 0.8680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier = KerasClassifier(build_fn=build_classifier, batch_size = 6, nb_epoch = 3)\n",
    "accuracies = cross_val_score(estimator=classifier, X=X_trainNN, y = y_trainNN, cv=10, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8681617379188538"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0010314051042729282"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Out Regularization\n",
    "# You can detect overfitting with the difference between test and train error or the high variance in cross-validation.\n",
    "\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "# first step: create a Sequential object, as a sequence of layers. B/C NN is a sequence of layers.\n",
    "classifier = Sequential()\n",
    "\n",
    "# add the first hidden layer\n",
    "classifier.add(Dense(units=5,kernel_initializer='glorot_uniform',\n",
    "                    activation = 'relu'))\n",
    "classifier.add(Dropout(0.2))  #often start with 0.1, not solved go up\n",
    "\n",
    "# add the second hidden layer\n",
    "classifier.add(Dense(units=5,kernel_initializer='glorot_uniform',\n",
    "                    activation = 'relu'))\n",
    "classifier.add(Dropout(0.2))  #often start with 0.1, not solved go up\n",
    "\n",
    "# add the output layer\n",
    "classifier.add(Dense(units=1,kernel_initializer='glorot_uniform',\n",
    "                    activation = 'sigmoid'))\n",
    "# compiling the NN\n",
    "classifier.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tuning with Grid Search\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def build_classifier(optimizer):\n",
    "    # first step: create a Sequential object, as a sequence of layers. B/C NN is a sequence of layers.\n",
    "    classifier = Sequential()\n",
    "    # add the first hidden layer\n",
    "    classifier.add(Dense(units=5,kernel_initializer='glorot_uniform',\n",
    "                    activation = 'relu'))\n",
    "    # add the second hidden layer\n",
    "    classifier.add(Dense(units=5,kernel_initializer='glorot_uniform',\n",
    "                    activation = 'relu'))\n",
    "    # add the output layer\n",
    "    classifier.add(Dense(units=1,kernel_initializer='glorot_uniform',\n",
    "                    activation = 'sigmoid'))\n",
    "    # compiling the NN\n",
    "    classifier.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['acc'])\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1627680 samples\n",
      "1627680/1627680 [==============================] - ETA: 0s - loss: 0.3600 - acc: 0.867 - 72s 44us/sample - loss: 0.3600 - acc: 0.8675\n",
      "Train on 1627680 samples\n",
      "1627680/1627680 [==============================] - 71s 44us/sample - loss: 0.3590 - acc: 0.8683\n",
      "Train on 1627680 samples\n",
      "1627680/1627680 [==============================] - 71s 43us/sample - loss: 0.3595 - acc: 0.8682\n",
      "Train on 1627680 samples\n",
      "1627680/1627680 [==============================] - 80s 49us/sample - loss: 0.3590 - acc: 0.8680\n",
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 70s 43us/sample - loss: 0.3593 - acc: 0.8682\n",
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 67s 41us/sample - loss: 0.3591 - acc: 0.8678\n",
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 70s 43us/sample - loss: 0.3601 - acc: 0.8678\n",
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 68s 42us/sample - loss: 0.3586 - acc: 0.8681\n",
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 67s 41us/sample - loss: 0.3589 - acc: 0.8684\n",
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 67s 41us/sample - loss: 0.3597 - acc: 0.8679\n",
      "Train on 1627680 samples\n",
      "1627680/1627680 [==============================] - 71s 43us/sample - loss: 0.3607 - acc: 0.8679\n",
      "Train on 1627680 samples\n",
      "1627680/1627680 [==============================] - 77s 47us/sample - loss: 0.3605 - acc: 0.8683\n",
      "Train on 1627680 samples\n",
      "1627680/1627680 [==============================] - 146s 90us/sample - loss: 0.3604 - acc: 0.8680\n",
      "Train on 1627680 samples\n",
      "1627680/1627680 [==============================] - 148s 91us/sample - loss: 0.3603 - acc: 0.8682\n",
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 145s 89us/sample - loss: 0.3612 - acc: 0.8681\n",
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 86s 53us/sample - loss: 0.3606 - acc: 0.8681\n",
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 73s 45us/sample - loss: 0.3603 - acc: 0.8684\n",
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 74s 45us/sample - loss: 0.3603 - acc: 0.8678\n",
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 73s 45us/sample - loss: 0.3602 - acc: 0.8684\n",
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 73s 45us/sample - loss: 0.3605 - acc: 0.8681\n",
      "Train on 1627680 samples\n",
      "1627680/1627680 [==============================] - 72s 44us/sample - loss: 0.3589 - acc: 0.8683\n",
      "Train on 1627680 samples\n",
      "1627680/1627680 [==============================] - 71s 44us/sample - loss: 0.3643 - acc: 0.8669\n",
      "Train on 1627680 samples\n",
      "1627680/1627680 [==============================] - 72s 44us/sample - loss: 0.3592 - acc: 0.8676\n",
      "Train on 1627680 samples\n",
      "1627680/1627680 [==============================] - 72s 44us/sample - loss: 0.3594 - acc: 0.8675\n",
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - ETA: 0s - loss: 0.3598 - acc: 0.867 - 71s 44us/sample - loss: 0.3598 - acc: 0.8677\n",
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 71s 44us/sample - loss: 0.3591 - acc: 0.8683\n",
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 72s 44us/sample - loss: 0.3591 - acc: 0.8681\n",
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 72s 44us/sample - loss: 0.3590 - acc: 0.8681\n",
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 72s 44us/sample - loss: 0.3589 - acc: 0.8684\n",
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 72s 44us/sample - loss: 0.3598 - acc: 0.8677\n",
      "Train on 1627680 samples\n",
      "1627680/1627680 [==============================] - 73s 45us/sample - loss: 0.3620 - acc: 0.8682\n",
      "Train on 1627680 samples\n",
      "1627680/1627680 [==============================] - 73s 45us/sample - loss: 0.3611 - acc: 0.8682\n",
      "Train on 1627680 samples\n",
      "1627680/1627680 [==============================] - 72s 44us/sample - loss: 0.3606 - acc: 0.8679\n",
      "Train on 1627680 samples\n",
      "1627680/1627680 [==============================] - 72s 45us/sample - loss: 0.3609 - acc: 0.8682\n",
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 73s 45us/sample - loss: 0.3605 - acc: 0.8681\n",
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 72s 44us/sample - loss: 0.3602 - acc: 0.8681\n",
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 72s 44us/sample - loss: 0.3602 - acc: 0.8681\n",
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 72s 44us/sample - loss: 0.3606 - acc: 0.8681\n",
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 73s 45us/sample - loss: 0.3606 - acc: 0.8680\n",
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 76s 47us/sample - loss: 0.3608 - acc: 0.8682\n",
      "Train on 1627680 samples\n",
      "1627680/1627680 [==============================] - 53s 32us/sample - loss: 0.3599 - acc: 0.8680\n",
      "Train on 1627680 samples\n",
      "1627680/1627680 [==============================] - 53s 32us/sample - loss: 0.3590 - acc: 0.8678\n",
      "Train on 1627680 samples\n",
      "1627680/1627680 [==============================] - 53s 33us/sample - loss: 0.3598 - acc: 0.8674\n",
      "Train on 1627680 samples\n",
      "1627680/1627680 [==============================] - 53s 32us/sample - loss: 0.3597 - acc: 0.8679\n",
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 56s 35us/sample - loss: 0.3601 - acc: 0.8679\n",
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 56s 34us/sample - loss: 0.3591 - acc: 0.8679\n",
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 56s 34us/sample - loss: 0.3594 - acc: 0.8684\n",
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 56s 34us/sample - loss: 0.3605 - acc: 0.8672\n",
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 56s 34us/sample - loss: 0.3599 - acc: 0.8679\n",
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 56s 35us/sample - loss: 0.3602 - acc: 0.8677\n",
      "Train on 1627680 samples\n",
      "1627680/1627680 [==============================] - 54s 33us/sample - loss: 0.3603 - acc: 0.8681s\n",
      "Train on 1627680 samples\n",
      "1627680/1627680 [==============================] - 54s 33us/sample - loss: 0.3625 - acc: 0.8672\n",
      "Train on 1627680 samples\n",
      "1627680/1627680 [==============================] - 55s 34us/sample - loss: 0.3651 - acc: 0.8667\n",
      "Train on 1627680 samples\n",
      "1627680/1627680 [==============================] - 55s 34us/sample - loss: 0.3615 - acc: 0.8680\n",
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 58s 36us/sample - loss: 0.3612 - acc: 0.8674\n",
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 57s 35us/sample - loss: 0.3599 - acc: 0.8681\n",
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 57s 35us/sample - loss: 0.3634 - acc: 0.8677\n",
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 58s 36us/sample - loss: 0.3602 - acc: 0.8681\n",
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 58s 35us/sample - loss: 0.3633 - acc: 0.8666\n",
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 58s 36us/sample - loss: 0.3608 - acc: 0.8681\n",
      "Train on 1627680 samples\n",
      "1627680/1627680 [==============================] - 53s 33us/sample - loss: 0.3593 - acc: 0.8682\n",
      "Train on 1627680 samples\n",
      "1627680/1627680 [==============================] - 53s 32us/sample - loss: 0.3596 - acc: 0.8681\n",
      "Train on 1627680 samples\n",
      "1627680/1627680 [==============================] - 53s 32us/sample - loss: 0.3590 - acc: 0.8681\n",
      "Train on 1627680 samples\n",
      "1627680/1627680 [==============================] - 53s 32us/sample - loss: 0.3589 - acc: 0.8683\n",
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 57s 35us/sample - loss: 0.3597 - acc: 0.8680\n",
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 56s 35us/sample - loss: 0.3606 - acc: 0.8671\n",
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 55s 34us/sample - loss: 0.3600 - acc: 0.8675\n",
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 59s 37us/sample - loss: 0.3591 - acc: 0.8682\n",
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 55s 34us/sample - loss: 0.3586 - acc: 0.8686\n",
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 54s 33us/sample - loss: 0.3598 - acc: 0.8680\n",
      "Train on 1627680 samples\n",
      "1627680/1627680 [==============================] - 53s 33us/sample - loss: 0.3607 - acc: 0.8677\n",
      "Train on 1627680 samples\n",
      "1627680/1627680 [==============================] - 54s 33us/sample - loss: 0.3658 - acc: 0.8668\n",
      "Train on 1627680 samples\n",
      "1627680/1627680 [==============================] - 53s 32us/sample - loss: 0.3606 - acc: 0.8676\n",
      "Train on 1627680 samples\n",
      "1627680/1627680 [==============================] - 53s 33us/sample - loss: 0.3598 - acc: 0.8682\n",
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 56s 34us/sample - loss: 0.3606 - acc: 0.8676s - l\n",
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 55s 34us/sample - loss: 0.3608 - acc: 0.8677\n",
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 57s 35us/sample - loss: 0.3613 - acc: 0.8674\n",
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 53s 33us/sample - loss: 0.3616 - acc: 0.8672\n",
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 55s 34us/sample - loss: 0.3607 - acc: 0.8683\n",
      "Train on 1627681 samples\n",
      "1627681/1627681 [==============================] - 56s 35us/sample - loss: 0.3603 - acc: 0.8681\n",
      "Train on 1808534 samples\n",
      "1808534/1808534 [==============================] - 63s 35us/sample - loss: 0.3602 - acc: 0.8679s - loss: 0.3\n"
     ]
    }
   ],
   "source": [
    "classifier = KerasClassifier(build_fn=build_classifier)\n",
    "\n",
    "# create a dictionary of hyper-parameters to optimize\n",
    "parameters = {'batch_size':[25,32], 'nb_epoch':[1,2],'optimizer':['adam','rmsprop']}\n",
    "grid_search = GridSearchCV(estimator = classifier, param_grid = parameters, scoring = 'accuracy', cv=10)\n",
    "grid_search = grid_search.fit(X_trainNN,y_trainNN)\n",
    "\n",
    "best_parameters = grid_search.best_params_ \n",
    "best_accuracy = grid_search.best_score_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
